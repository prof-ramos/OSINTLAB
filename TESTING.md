# ğŸ§ª OSINTLAB - Guia de Testes

## VisÃ£o Geral

Este guia descreve o sistema completo de testes do OSINTLAB, incluindo estrutura, orquestraÃ§Ã£o e execuÃ§Ã£o.

## ğŸ“ Estrutura de Testes

```
tests/
â”œâ”€â”€ __init__.py                 # InicializaÃ§Ã£o do pacote de testes
â”œâ”€â”€ conftest.py                 # ConfiguraÃ§Ãµes globais e fixtures
â”œâ”€â”€ pytest_orchestration.py     # Sistema de orquestraÃ§Ã£o inteligente
â”œâ”€â”€ unit/                       # Testes unitÃ¡rios (rÃ¡pidos, isolados)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_domain_checker.py
â”œâ”€â”€ integration/                # Testes de integraÃ§Ã£o (APIs, banco, etc)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_domain_checker_integration.py
â””â”€â”€ e2e/                        # Testes end-to-end (fluxo completo)
    â””â”€â”€ __init__.py
```

## ğŸ·ï¸ Marcadores de Testes

Os testes sÃ£o organizados usando marcadores (markers) do pytest:

- **`@pytest.mark.unit`**: Testes unitÃ¡rios rÃ¡pidos e isolados
- **`@pytest.mark.integration`**: Testes de integraÃ§Ã£o entre componentes
- **`@pytest.mark.e2e`**: Testes end-to-end completos
- **`@pytest.mark.fast`**: Testes que executam em < 1 segundo
- **`@pytest.mark.slow`**: Testes que demoram > 5 segundos
- **`@pytest.mark.network`**: Testes que requerem conectividade de rede
- **`@pytest.mark.asyncio`**: Testes assÃ­ncronos
- **`@pytest.mark.benchmark`**: Testes de performance

## ğŸš€ Como Executar os Testes

### Usando Make (Recomendado)

```bash
# Executar todos os testes (modo otimizado)
make test

# Testes unitÃ¡rios apenas
make test-unit

# Testes unitÃ¡rios rÃ¡pidos
make test-unit-fast

# Testes de integraÃ§Ã£o
make test-integration

# Testes em paralelo
make test-parallel

# Testes com cobertura detalhada
make test-coverage

# Testes de benchmark
make test-benchmark
```

### Usando Script Shell

```bash
# Executar todos os testes
./scripts/run_tests.sh all

# Testes unitÃ¡rios
./scripts/run_tests.sh unit

# Testes rÃ¡pidos
./scripts/run_tests.sh fast

# Testes com cobertura
./scripts/run_tests.sh coverage

# Pipeline de CI
./scripts/run_tests.sh ci
```

### Usando Pytest Diretamente

```bash
# Todos os testes
pytest tests/ -v

# Testes unitÃ¡rios
pytest tests/unit -v

# Testes de integraÃ§Ã£o
pytest tests/integration -v

# Testes por marcador
pytest tests/ -v -m "unit and fast"
pytest tests/ -v -m "not slow"

# Testes paralelos
pytest tests/ -n auto -v

# Com cobertura
pytest tests/ -v --cov=tools --cov=ui --cov-report=html
```

## ğŸ¯ Sistema de OrquestraÃ§Ã£o

O sistema de orquestraÃ§Ã£o inteligente (`pytest_orchestration.py`) oferece:

### Modos de ExecuÃ§Ã£o

#### 1. Modo Sequencial
Executa testes um apÃ³s o outro.

```bash
python tests/pytest_orchestration.py --mode sequential
```

#### 2. Modo Paralelo
Executa todos os testes em paralelo com workers automÃ¡ticos.

```bash
python tests/pytest_orchestration.py --mode parallel
```

#### 3. Modo Condicional
Executa testes rÃ¡pidos primeiro; se passarem, executa os lentos.

```bash
python tests/pytest_orchestration.py --mode conditional
```

#### 4. Modo Otimizado (PadrÃ£o)
ExecuÃ§Ã£o inteligente com balanceamento de recursos.

```bash
python tests/pytest_orchestration.py --mode optimized
# ou simplesmente
python tests/pytest_orchestration.py
```

### Categorias EspecÃ­ficas

```bash
# Apenas testes unitÃ¡rios
python tests/pytest_orchestration.py --category unit

# Apenas integraÃ§Ã£o
python tests/pytest_orchestration.py --category integration

# Todos os testes
python tests/pytest_orchestration.py --category all
```

## ğŸ“Š RelatÃ³rios

### RelatÃ³rio de Cobertura HTML

```bash
make test-coverage
make report  # Abre o relatÃ³rio no navegador
```

### RelatÃ³rio JSON

ApÃ³s execuÃ§Ã£o, os relatÃ³rios JSON sÃ£o gerados em:
- `coverage.json` - Cobertura de cÃ³digo
- `htmlcov/pytest_report.json` - Resultados dos testes
- `htmlcov/orchestration_report.json` - RelatÃ³rio de orquestraÃ§Ã£o

### Visualizar RelatÃ³rios

```bash
# RelatÃ³rio de cobertura
make report

# RelatÃ³rio do pytest
make report-pytest
```

## ğŸ”§ ConfiguraÃ§Ã£o

### pyproject.toml

Toda a configuraÃ§Ã£o de testes estÃ¡ em `pyproject.toml`:

```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
markers = [
    "unit: Testes unitÃ¡rios",
    "integration: Testes de integraÃ§Ã£o",
    # ... outros marcadores
]
```

### Fixtures Globais

As fixtures compartilhadas estÃ£o em `tests/conftest.py`:

- `project_root`: DiretÃ³rio raiz do projeto
- `aio_session`: SessÃ£o aiohttp para testes
- `mock_aio_session`: Mock de sessÃ£o aiohttp
- `test_domains`: Lista de domÃ­nios para testes
- `performance_tracker`: Rastreador de performance

## ğŸ¨ Boas PrÃ¡ticas

### 1. Nomenclatura

```python
# Arquivos de teste
test_*.py ou *_test.py

# Classes de teste
class TestNomeDaFuncionalidade:
    pass

# FunÃ§Ãµes de teste
def test_comportamento_esperado():
    pass
```

### 2. OrganizaÃ§Ã£o

```python
# Arrange (Preparar)
dados = preparar_dados()

# Act (Agir)
resultado = funcao_a_testar(dados)

# Assert (Verificar)
assert resultado == esperado
```

### 3. Marcadores

```python
@pytest.mark.unit
@pytest.mark.fast
def test_funcao_rapida():
    assert True

@pytest.mark.integration
@pytest.mark.network
@pytest.mark.slow
async def test_api_externa():
    # Teste que faz chamada de rede
    pass
```

### 4. Fixtures

```python
@pytest.fixture
def dados_de_teste():
    return {"key": "value"}

def test_com_fixture(dados_de_teste):
    assert dados_de_teste["key"] == "value"
```

## ğŸ”„ CI/CD

### GitHub Actions

O workflow `.github/workflows/tests.yml` executa:

1. âœ… Testes unitÃ¡rios (todas as plataformas e versÃµes Python)
2. âœ… Testes de integraÃ§Ã£o
3. ğŸš€ Testes paralelos otimizados
4. ğŸ” VerificaÃ§Ãµes de qualidade (linting, formataÃ§Ã£o)
5. â±ï¸ Benchmarks de performance
6. ğŸ“Š Upload de cobertura para Codecov

### Executar Localmente

```bash
# Simular pipeline de CI
make ci

# Ou usando script
./scripts/run_tests.sh ci
```

## ğŸ“ˆ Monitoramento de Performance

### Benchmarks

```bash
# Executar benchmarks
make test-benchmark

# Ou
pytest tests/ -v -m benchmark --benchmark-only
```

### Profiling

```python
# Usar fixture de performance
def test_com_profiling(performance_tracker):
    with performance_tracker:
        # CÃ³digo a medir
        funcao_lenta()

    print(f"DuraÃ§Ã£o: {performance_tracker.duration}s")
```

## ğŸ› Debugging

### Modo Verbose

```bash
pytest tests/ -vv  # Muito verbose
```

### Parar no Primeiro Erro

```bash
pytest tests/ -x  # Fail fast
```

### Executar Teste EspecÃ­fico

```bash
pytest tests/unit/test_domain_checker.py::TestGenerateDomains::test_generate_2letters -v
```

### PDB (Python Debugger)

```bash
pytest tests/ --pdb  # Para no erro
pytest tests/ -s     # Mostra prints
```

## ğŸ“¦ DependÃªncias de Teste

Instaladas via `uv`:

```bash
# Apenas dependÃªncias de teste
uv pip install -e ".[test]"

# Todas as dependÃªncias de desenvolvimento
uv pip install -e ".[dev]"

# Tudo
uv pip install -e ".[all]"
```

## ğŸ” Troubleshooting

### Testes Falhando

1. Verifique dependÃªncias:
   ```bash
   make install
   ```

2. Limpe cache:
   ```bash
   make clean
   ```

3. Execute testes especÃ­ficos:
   ```bash
   pytest tests/unit -v
   ```

### Performance Lenta

1. Use testes paralelos:
   ```bash
   make test-parallel
   ```

2. Execute apenas testes rÃ¡pidos:
   ```bash
   make test-unit-fast
   ```

3. Use modo otimizado:
   ```bash
   python tests/pytest_orchestration.py --mode optimized
   ```

## ğŸ“š Recursos Adicionais

- [Pytest Documentation](https://docs.pytest.org/)
- [pytest-xdist](https://pytest-xdist.readthedocs.io/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)
- [pytest-asyncio](https://pytest-asyncio.readthedocs.io/)

## ğŸ¤ Contribuindo

Ao adicionar novos testes:

1. Siga a estrutura de diretÃ³rios
2. Use marcadores apropriados
3. Adicione fixtures em `conftest.py` se reutilizÃ¡veis
4. Documente testes complexos
5. Execute `make pre-commit` antes de commitar

---

**Criado por:** Gabriel Ramos
**VersÃ£o:** 1.0.0
**Ãšltima atualizaÃ§Ã£o:** 2025-11-06
