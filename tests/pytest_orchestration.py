"""
Sistema de Orquestra√ß√£o de Testes
==================================

Gerencia execu√ß√£o inteligente de testes com otimiza√ß√£o de recursos e an√°lise de performance.
"""

import sys
import json
import time
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional
from enum import Enum
import subprocess


class TestCategory(Enum):
    """Categorias de testes"""
    UNIT = "unit"
    INTEGRATION = "integration"
    E2E = "e2e"
    ALL = "all"


class ExecutionMode(Enum):
    """Modos de execu√ß√£o"""
    SEQUENTIAL = "sequential"
    PARALLEL = "parallel"
    CONDITIONAL = "conditional"
    OPTIMIZED = "optimized"


@dataclass
class TestExecutionConfig:
    """Configura√ß√£o de execu√ß√£o de testes"""
    category: TestCategory
    mode: ExecutionMode
    workers: int = -1  # -1 = auto
    timeout: int = 300
    failfast: bool = False
    verbose: bool = True
    coverage: bool = True
    html_report: bool = True
    json_report: bool = True
    markers: Optional[List[str]] = None


@dataclass
class TestExecutionResult:
    """Resultado da execu√ß√£o de testes"""
    category: str
    mode: str
    total_tests: int
    passed: int
    failed: int
    skipped: int
    errors: int
    duration: float
    coverage_percentage: Optional[float] = None
    exit_code: int = 0


class TestOrchestrator:
    """
    Orquestrador de Testes Inteligente

    Gerencia execu√ß√£o de testes com:
    - Execu√ß√£o paralela otimizada
    - An√°lise de depend√™ncias
    - Monitoramento de recursos
    - Gera√ß√£o de relat√≥rios
    """

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.tests_dir = project_root / "tests"
        self.results: List[TestExecutionResult] = []

    def build_pytest_command(self, config: TestExecutionConfig) -> List[str]:
        """
        Constr√≥i comando pytest baseado na configura√ß√£o

        Args:
            config: Configura√ß√£o de execu√ß√£o

        Returns:
            Lista com comando e argumentos
        """
        cmd = ["pytest"]

        # Diret√≥rio de testes baseado na categoria
        if config.category == TestCategory.UNIT:
            cmd.append(str(self.tests_dir / "unit"))
        elif config.category == TestCategory.INTEGRATION:
            cmd.append(str(self.tests_dir / "integration"))
        elif config.category == TestCategory.E2E:
            cmd.append(str(self.tests_dir / "e2e"))
        else:
            cmd.append(str(self.tests_dir))

        # Execu√ß√£o paralela
        if config.mode in [ExecutionMode.PARALLEL, ExecutionMode.OPTIMIZED]:
            if config.workers == -1:
                cmd.extend(["-n", "auto"])
            else:
                cmd.extend(["-n", str(config.workers)])

        # Verbose
        if config.verbose:
            cmd.append("-v")

        # Coverage
        if config.coverage:
            cmd.extend([
                "--cov=tools",
                "--cov=ui",
                "--cov-report=term-missing",
                "--cov-report=html",
                "--cov-report=json",
            ])

        # Fail fast
        if config.failfast:
            cmd.append("-x")

        # Timeout
        cmd.extend(["--timeout", str(config.timeout)])

        # HTML Report
        if config.html_report:
            cmd.extend([
                "--html=htmlcov/pytest_report.html",
                "--self-contained-html"
            ])

        # JSON Report
        if config.json_report:
            cmd.extend([
                "--json-report",
                "--json-report-file=htmlcov/pytest_report.json"
            ])

        # Marcadores customizados
        if config.markers:
            for marker in config.markers:
                cmd.extend(["-m", marker])

        return cmd

    def execute_tests(self, config: TestExecutionConfig) -> TestExecutionResult:
        """
        Executa testes com a configura√ß√£o especificada

        Args:
            config: Configura√ß√£o de execu√ß√£o

        Returns:
            Resultado da execu√ß√£o
        """
        print(f"\n{'=' * 80}")
        print(f"üß™ Executando testes: {config.category.value}")
        print(f"‚öôÔ∏è  Modo: {config.mode.value}")
        print(f"{'=' * 80}\n")

        cmd = self.build_pytest_command(config)
        print(f"üìù Comando: {' '.join(cmd)}\n")

        start_time = time.time()

        try:
            result = subprocess.run(
                cmd,
                cwd=str(self.project_root),
                capture_output=False,
                text=True
            )
            exit_code = result.returncode
        except Exception as e:
            print(f"‚ùå Erro ao executar testes: {e}")
            exit_code = 1

        duration = time.time() - start_time

        # Analisa resultados do JSON report
        test_result = self._parse_results(config, duration, exit_code)

        self.results.append(test_result)
        return test_result

    def _parse_results(
        self,
        config: TestExecutionConfig,
        duration: float,
        exit_code: int
    ) -> TestExecutionResult:
        """
        Analisa resultados dos testes

        Args:
            config: Configura√ß√£o de execu√ß√£o
            duration: Dura√ß√£o da execu√ß√£o
            exit_code: C√≥digo de sa√≠da

        Returns:
            Resultado estruturado
        """
        # Tenta ler o relat√≥rio JSON
        json_report_path = self.project_root / "htmlcov" / "pytest_report.json"
        coverage_json_path = self.project_root / "coverage.json"

        total_tests = 0
        passed = 0
        failed = 0
        skipped = 0
        errors = 0
        coverage_percentage = None

        if json_report_path.exists():
            try:
                with open(json_report_path) as f:
                    report = json.load(f)
                    summary = report.get("summary", {})
                    total_tests = summary.get("total", 0)
                    passed = summary.get("passed", 0)
                    failed = summary.get("failed", 0)
                    skipped = summary.get("skipped", 0)
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao ler relat√≥rio JSON: {e}")

        if coverage_json_path.exists() and config.coverage:
            try:
                with open(coverage_json_path) as f:
                    coverage = json.load(f)
                    coverage_percentage = coverage.get("totals", {}).get("percent_covered")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao ler cobertura: {e}")

        return TestExecutionResult(
            category=config.category.value,
            mode=config.mode.value,
            total_tests=total_tests,
            passed=passed,
            failed=failed,
            skipped=skipped,
            errors=errors,
            duration=duration,
            coverage_percentage=coverage_percentage,
            exit_code=exit_code
        )

    def execute_pipeline(
        self,
        mode: ExecutionMode = ExecutionMode.OPTIMIZED
    ) -> List[TestExecutionResult]:
        """
        Executa pipeline completo de testes

        Args:
            mode: Modo de execu√ß√£o

        Returns:
            Lista de resultados
        """
        print("\n" + "=" * 80)
        print("üöÄ INICIANDO PIPELINE DE TESTES")
        print("=" * 80)

        # Execu√ß√£o condicional: testes r√°pidos primeiro
        if mode == ExecutionMode.CONDITIONAL:
            # 1. Testes unit√°rios r√°pidos
            unit_config = TestExecutionConfig(
                category=TestCategory.UNIT,
                mode=ExecutionMode.PARALLEL,
                markers=["unit and fast"],
                failfast=True
            )
            unit_result = self.execute_tests(unit_config)

            # Se testes unit√°rios passarem, executa integra√ß√£o
            if unit_result.exit_code == 0:
                integration_config = TestExecutionConfig(
                    category=TestCategory.INTEGRATION,
                    mode=ExecutionMode.PARALLEL,
                    markers=["integration"]
                )
                self.execute_tests(integration_config)
            else:
                print("\n‚ö†Ô∏è Testes unit√°rios falharam. Pulando testes de integra√ß√£o.")

        # Execu√ß√£o otimizada: paralelo com categoriza√ß√£o
        elif mode == ExecutionMode.OPTIMIZED:
            # Testes unit√°rios em paralelo
            unit_config = TestExecutionConfig(
                category=TestCategory.UNIT,
                mode=ExecutionMode.PARALLEL,
                workers=-1
            )
            self.execute_tests(unit_config)

            # Testes de integra√ß√£o em paralelo (menos workers por serem I/O bound)
            integration_config = TestExecutionConfig(
                category=TestCategory.INTEGRATION,
                mode=ExecutionMode.PARALLEL,
                workers=4
            )
            self.execute_tests(integration_config)

        # Execu√ß√£o sequencial
        elif mode == ExecutionMode.SEQUENTIAL:
            for category in [TestCategory.UNIT, TestCategory.INTEGRATION]:
                config = TestExecutionConfig(
                    category=category,
                    mode=ExecutionMode.SEQUENTIAL
                )
                self.execute_tests(config)

        # Execu√ß√£o paralela de tudo
        else:
            config = TestExecutionConfig(
                category=TestCategory.ALL,
                mode=ExecutionMode.PARALLEL
            )
            self.execute_tests(config)

        return self.results

    def generate_summary_report(self):
        """Gera relat√≥rio resumido da execu√ß√£o"""
        print("\n" + "=" * 80)
        print("üìä RELAT√ìRIO FINAL DE EXECU√á√ÉO")
        print("=" * 80 + "\n")

        total_duration = sum(r.duration for r in self.results)
        total_tests = sum(r.total_tests for r in self.results)
        total_passed = sum(r.passed for r in self.results)
        total_failed = sum(r.failed for r in self.results)
        total_skipped = sum(r.skipped for r in self.results)

        for result in self.results:
            status = "‚úÖ PASSOU" if result.exit_code == 0 else "‚ùå FALHOU"
            print(f"\n{status} - {result.category.upper()}")
            print(f"  Testes: {result.total_tests} | "
                  f"‚úÖ {result.passed} | "
                  f"‚ùå {result.failed} | "
                  f"‚è≠Ô∏è {result.skipped}")
            print(f"  Dura√ß√£o: {result.duration:.2f}s")
            if result.coverage_percentage:
                print(f"  Cobertura: {result.coverage_percentage:.2f}%")

        print("\n" + "=" * 80)
        print(f"üéØ TOTAL: {total_tests} testes em {total_duration:.2f}s")
        print(f"‚úÖ Passed: {total_passed}")
        print(f"‚ùå Failed: {total_failed}")
        print(f"‚è≠Ô∏è Skipped: {total_skipped}")

        success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        print(f"üìà Taxa de Sucesso: {success_rate:.1f}%")
        print("=" * 80 + "\n")

        # Salva relat√≥rio em JSON
        report_path = self.project_root / "htmlcov" / "orchestration_report.json"
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with open(report_path, 'w') as f:
            json.dump(
                {
                    "summary": {
                        "total_duration": total_duration,
                        "total_tests": total_tests,
                        "total_passed": total_passed,
                        "total_failed": total_failed,
                        "total_skipped": total_skipped,
                        "success_rate": success_rate
                    },
                    "results": [asdict(r) for r in self.results]
                },
                f,
                indent=2
            )

        print(f"üíæ Relat√≥rio salvo em: {report_path}")


def main():
    """Fun√ß√£o principal"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Sistema de Orquestra√ß√£o de Testes OSINTLAB"
    )
    parser.add_argument(
        "--mode",
        choices=["sequential", "parallel", "conditional", "optimized"],
        default="optimized",
        help="Modo de execu√ß√£o (padr√£o: optimized)"
    )
    parser.add_argument(
        "--category",
        choices=["unit", "integration", "e2e", "all"],
        help="Categoria espec√≠fica de testes"
    )

    args = parser.parse_args()

    project_root = Path(__file__).parent.parent
    orchestrator = TestOrchestrator(project_root)

    mode = ExecutionMode(args.mode)

    if args.category:
        # Executa apenas categoria espec√≠fica
        category = TestCategory(args.category)
        config = TestExecutionConfig(category=category, mode=mode)
        orchestrator.execute_tests(config)
    else:
        # Executa pipeline completo
        orchestrator.execute_pipeline(mode)

    orchestrator.generate_summary_report()

    # Exit code baseado nos resultados
    exit_code = max((r.exit_code for r in orchestrator.results), default=0)
    sys.exit(exit_code)


if __name__ == "__main__":
    main()
